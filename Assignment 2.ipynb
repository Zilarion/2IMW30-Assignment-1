{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Foundations of Data Mining: Assignment 2\n",
    "\n",
    "Please complete all assignments in this notebook. You should submit this notebook, as well as a PDF version (See File > Download as)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *\n",
    "plt.rcParams['savefig.dpi'] = 100 # This controls the size of your figures\n",
    "# Comment out and restart notebook if you only want the last output of each cell.\n",
    "InteractiveShell.ast_node_interactivity = \"none\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We load in our key by placing it in ~/.openml/config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Kernel selection (4 points (1+2+1))\n",
    "SVMs can be trained with different kernels. Generate a 2-dimensional dataset as shown below and study the effect of the choice of kernel by visualizing the results.\n",
    "\n",
    "- Train a SVM classifier on the dataset using respectively a linear, polynomial and radial basis function (RBF) kernel, evaluate the performance of each kernel using 10-fold cross-validation and AUC. Which one works best? Visualize the results. Can you intuitively explain why one kernel is more suited than another?\n",
    "    - Hint: you can use the visualization code used in class. It is under mglearn/plot_svm.py > plot_svm_kernels().\n",
    "- Take the RBF kernel and vary both the C parameter and the kernel width ($\\gamma$). Use 3 values for each (a very small, default, and very large value). For each of the 9 combinations, create the same RBF plot as before, report the number of support vectors, and the AUC performance. Explain the performance results. When are you over/underfitting?\n",
    "    - Hint: values for C and $\\gamma$ are typically in [$2^{-15}..2^{15}$] on a log scale. \n",
    "    - Hint: don't count the support vectors manually, retrieve them from the trained SVM.\n",
    "- Vary C and $\\gamma$ again, but this time use a grid of at least 20x20, vary both parameters uniformly on a log scale, and visualise the results using a $C \\times \\gamma \\rightarrow AUC$ heatmap. Explain the performance results, and compare them to the 9 results obtained in the previous subquestion. Can you also tell in which regions of the heatmap you are over/underfitting?\n",
    "    - Hint: We've constructed such a heatmap in class and in assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(centers=2, n_samples=1000, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Robots and SVMs (4 points (2+1+1))\n",
    "\n",
    "The [Wall Robot Navigation dataset](http://www.openml.org/d/1497) contains about 5500 readings of an ultrasound sensor array mounted on a robot, and your task is to finetune and train an SVM classifier to predict how the robot should move next.\n",
    "\n",
    "- Make a stratified 80-20 split of the data. On the training set alone, optimize the main hyperparameters of the SVM for Accuracy with a random search. Vary at least the main kernel types (linear, polynomial, and RBF), the C parameter, the $\\gamma$ parameter for the RBF kernel and the exponent/degree for the polynomial kernel. Report the optimal hyperparameter settings and Accuracy performance. \n",
    "    - The degree of the polynonial is typically in the range 2..10.\n",
    "    - Hint: note that the hyperparameter ranges depend on each other. For instance, $\\gamma$ only makes sense if you have selected the RBF kernel as well. We've seen in class how to define multiple hyperparameter spaces in a random/grid search.\n",
    "- Use a 5x3-fold (5 outer, 3 inner) nested cross-validation (CV) on the training set to obtain a clean evaluation. Evaluate your optimized hyperparameter settings on the separate test set and discuss the result. Is the performance on the independent test set comparable with the result of the random search?\n",
    "    - Hint: for the nested resampling, use at least a 10-fold CV for the outer loop. The inner loop can be a 3-fold CV or a simple holdout.\n",
    "- Train an SVM using the optimal hyperparameter configuration you found and test it on the held out (20%) test set. Compare this Accuracy result with the (mean) result of the nested CV. If you would build this robot in practice, how would you find the hyperparameters to use, and which performance would you expect? Is it truly necessary to tune the hyperparameters? Which hyperparameters were most important to tune?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "robot_data = oml.datasets.get_dataset(1497) # Download Robot data\n",
    "# Get the predictors X and the labels y\n",
    "X, y = robot_data.get_data(target=robot_data.default_target_attribute); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## A benchmark study (3 points (2+1))\n",
    "\n",
    "A benchmark study is an experiment in which multiple algorithms are evaluated on multiple datasets. The end goal is to study whether one algorithm is generally better than the others. Meaningful benchmark studies can grow quite complex, here we do a simplified variant.\n",
    "\n",
    "* Download OpenML datasets 37, 470, 1120, 1464 and 1471. They are sufficiently large (e.g., at least 500 data points) so that the performance estimation is trustworthy. Select at least three classifiers that we discussed in class, e.g. kNN, Logistic Regression, Random Forests, Gradient Boosting, SVMs, Naive Bayes. Note that some of these algorithms take longer to train. Evaluate all classifiers (with default parameter settings) on all datasets, using a 10-fold CV and AUC. Show the results in a table and interpret them. Which is the best algorithm in this benchmark?\n",
    "    * Note that these datasets have categorical features, different scales, missing values, and (likely) irrelevant features. You'll need to build pipelines to correctly build all models. Also remove any row identifiers (see, e.g., https://www.openml.org/d/1120)\n",
    "    * Hint: You can either compare the performances directly, or (better) use a statistical significance test, e.g. a pairwise t-test or (better) Wilcoxon signed ranks test, to see whether the performance differences are significant. This is covered in statistics courses. You can then count wins, ties and losses.\n",
    "* Repeat the benchmark, but now additionally optimize the main hyperparameters of each algorithm in a grid or random search (explore at least 5 values per hyperparameter, where possible). Does this affect the ranking of the algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test on dataset # 37\n",
      "Running test on dataset # 470\n",
      "Running test on dataset # 1120\n",
      "Running test on dataset # 1464\n",
      "Running test on dataset # 1471\n",
      "Default scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>37</th>\n",
       "      <th>470</th>\n",
       "      <th>1120</th>\n",
       "      <th>1464</th>\n",
       "      <th>1471</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        37    470   1120  1464  1471\n",
       "KNeighborsClassifier    0.78  0.58  0.88  0.53  0.47\n",
       "RandomForestClassifier  0.78  0.59  0.91  0.52  0.56\n",
       "SVC                     0.82  0.59  0.91  0.70  0.35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized grid scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>37</th>\n",
       "      <th>470</th>\n",
       "      <th>1120</th>\n",
       "      <th>1464</th>\n",
       "      <th>1471</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        37    470   1120  1464  1471\n",
       "KNeighborsClassifier    0.80  0.60  0.90  0.74  0.48\n",
       "RandomForestClassifier  0.81  0.62  0.93  0.58  0.58\n",
       "SVC                     0.82  0.60  0.92  0.71  0.43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import Imputer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define datasets and base classifiers\n",
    "datasets = [37, 470, 1120, 1464, 1471]\n",
    "\n",
    "# Create kf\n",
    "kf = StratifiedKFold(n_splits=10, random_state=2)\n",
    "\n",
    "# Create imputer to pre-process data\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "\n",
    "# Create feature selector\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "\n",
    "# Create scaler for the classifiers which need this\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pipelines = [\n",
    "    Pipeline([(\"imputer\",imp), (\"selector\", sel), (\"scaler\", scaler), (\"classifier\", neighbors.KNeighborsClassifier())]),\n",
    "    Pipeline([(\"imputer\",imp), (\"selector\", sel), (\"scaler\", scaler), (\"classifier\", svm.SVC())]),\n",
    "    Pipeline([(\"imputer\",imp), (\"selector\", sel), (\"classifier\", RandomForestClassifier())])\n",
    "]\n",
    "\n",
    "# Define parameter grids for the classifiers\n",
    "param_grids = [\n",
    "    {'classifier__n_neighbors': [1, 3, 5, 7, 9, 11, 33]}, \n",
    "    {'classifier__C': [0.001, 0.01, 1, 10, 100, 1000] },\n",
    "    {'classifier__max_features': list(range(1,5)), 'classifier__n_estimators': [1, 2, 4, 8, 16, 32, 64]}\n",
    "]\n",
    "\n",
    "# Score dicts\n",
    "dsScores = {}\n",
    "dsGridScores = {}\n",
    "\n",
    "# Process each dataset\n",
    "for dindex in datasets:\n",
    "    # Load data\n",
    "    data_set = oml.datasets.get_dataset(dindex)\n",
    "    X, y = data_set.get_data(target=data_set.default_target_attribute)\n",
    "    print(\"Running test on dataset #\", dindex)\n",
    "    \n",
    "    scores = {}\n",
    "    gridScores = {}\n",
    "    # Run each clf using the corresponding parameter grid\n",
    "    for pipe, grid in zip(pipelines, param_grids):\n",
    "        # Run 10-fold pipeline\n",
    "        score = cross_val_score(pipe, X, y, cv=kf, n_jobs=-1, scoring=\"roc_auc\").mean()\n",
    "        scores[pipe.named_steps[\"classifier\"].__class__.__name__] = score;\n",
    "\n",
    "        # Run gridsearch pipeline\n",
    "        gs = GridSearchCV(pipe, grid, cv=kf, n_jobs=-1, scoring=\"roc_auc\")\n",
    "        gs.fit(X, y)\n",
    "        results = pd.DataFrame(gs.cv_results_)\n",
    "        gridScores[pipe.named_steps[\"classifier\"].__class__.__name__] = gs.best_score_\n",
    "\n",
    "    dsScores[dindex] = scores\n",
    "    dsGridScores[dindex] = gridScores\n",
    "    \n",
    "# Print default classifier scores\n",
    "print(\"Default scores:\")\n",
    "display(pd.DataFrame.from_dict(dsScores))\n",
    "\n",
    "# Print best grid scores\n",
    "print(\"\\nOptimized grid scores:\")\n",
    "display(pd.DataFrame.from_dict(dsGridScores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default parameters, we note that the SVC is coming out on top on every test except for on dataset #1471. Although the random forest classifier is not far behind. The kNN classifier performs the lowest of all three classifiers.\n",
    "\n",
    "After optimization we note that the SVC no longer comes out on top on most tests, in fact, it is even surpassed by the kNN classifier.\n",
    "\n",
    "In fact, the Random forest classifier gets the best results on all datasets except for #1464 (and #37, but only by 1 percent).\n",
    "However, the reason the random forest classifier performs poorly on this dataset is likely contributed due to the low amount of features of this dataset (it only has 5). Our conclusion therefore is that the random forest classifer does better on higher amount of features, while on low dimensional datasets the SVC or kNN classifier is a better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gaussian Processes (2 points (1+1))\n",
    "\n",
    "Consider the RAM prices dataset (included in the data folder). Separate the data in a training set of all data points up until the year 2000, and a test set with all points after that.\n",
    "\n",
    "- Train several of the algorithms we have covered in the course that can handle regression. Include at least linear regression, decision tree, and RandomForest. Which ones give the best $R^2$ performance on the test set? Plot the predictions (both on the training and test data) on the figure below. Use different colors for different algorithms or build multiple plots.\n",
    "- Train a Gaussian process on an increasing amount of samples of the training data. Start with 5 random sample and plot the predictions (both the mean and the uncertainty interval) for both training and test data, as shown in class. Now add 5 more points and retrain and redraw. Do this a couple of times and interpret/explain what you see. Finally, train the Gaussian on the full dataset and again show plot the predictions. Evaluate on the test set using  $R^2$. Compare these results with those achieved with other algorithms and explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/pdf": "JVBERi0xLjQKJazcIKu6CjEgMCBvYmoKPDwgL1R5cGUgL0NhdGFsb2cgL1BhZ2VzIDIgMCBSID4+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\nT0YK\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAADRCAYAAAB1sYEuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nr1OnZgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1188e45c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "ram_prices = pd.read_csv('data/ram_price.csv')\n",
    "\n",
    "plt.semilogy(ram_prices.date, ram_prices.price)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price in $/Mbyte\");\n",
    "\n",
    "train = ram_prices.query(\"date < 2000\")\n",
    "test = ram_prices.query(\"date >= 2000\")\n",
    "\n",
    "x_train = train.date.reshape(-1, 1)\n",
    "y_train = train.price.reshape(-1, 1)\n",
    "x_test = test.date.reshape(-1, 1)\n",
    "y_test = test.price.reshape(-1, 1)\n",
    "\n",
    "# print (train.date.array)\n",
    "algorithms = [LinearRegression(), RandomForestRegressor(), DecisionTreeRegressor()]\n",
    "\n",
    "for alg in algorithms:\n",
    "    alg.fit(x_train, y_train)\n",
    "    y_result = alg.predict(ram_prices.date.reshape(-1, 1))\n",
    "    plt.semilogy(ram_prices.date.reshape(-1, 1), y_result)\n",
    "    \n",
    "for sampleSize in range(5, len(x_train), 5):\n",
    "    clf = GaussianProcessRegressor()\n",
    "    clf.fit(x_train[:sampleSize], y_train[:sampleSize])\n",
    "#     print(clf.score(x_test ,y_test))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## A mini-data mining challenge (2 points (+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The goal here is to use everything you have learned to build the best model for a given classification task. The task is hosted on OpenML, so you will receive the train-test splits, and your model will be evaluated on the server. The goal is to reasonably select algorithms and hyperparameter settings to obtain the best model. You can also do model selection and parameter optimization as you have done before. Skeleton code is provided in the OpenML tutorial.\n",
    "\n",
    "- All details can be found online:\n",
    "    - The OpenML Task ID is 145677: https://www.openml.org/t/145677\n",
    "    - The dataset description can be found here: https://www.openml.org/d/4134\n",
    "- A leaderboard is kept of the best models: https://www.openml.org/t/145677#!people\n",
    "    - You are able to see the solutions of others (by clicking in the timeline or run list), but resubmission of the exact same solution does not register on the leaderboard.\n",
    "    - You can share one account (one API key) per team. In case you use two, we take the one that performs best.\n",
    "- You can document the different experiments that you ran in this notebook. For each experiment, provide a description of how you chose the algorithms and parameters that you submitted. Try to reason about which experiments to try, don't just do an immense random search.\n",
    "- Points are rewarded as follows:\n",
    "    - 1 point for the breadth of experiments you ran (algorithms, hyperparameter settings)\n",
    "    - 1 point for reasoning/insight and interpretation of the results\n",
    "    - 1 (bonus) point for every team who has uploaded the best solution thus far **on AUC** (who reaches the top of the leaderboard at any moment during the assignment)\n",
    "        - Note: On the leaderboard page, the 'frontier' line is drawn, and your top ranking is also shown in the table.\n",
    "        \n",
    "Note: Report AUC scores in your report as well. In case of issues with OpenML we will use the experiments and scores mentioned your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import openml as oml\n",
    "\n",
    "# Load task data\n",
    "task = oml.tasks.get_task(145677)\n",
    "data = oml.datasets.get_dataset(task.dataset_id)\n",
    "X, y = data.get_data(target=data.default_target_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Initial ideas\n",
    "We have lots of features (1777) Hence, we should either figure something out to reduce the amount features, or use a classifier which handles a lot of features well. Initially, our thoughts fell on using a random forest classifier without selecting features (this is what the decision trees do themselves anyhow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running..\n",
      "Publishing run\n",
      "RF on Bioresponse: http://www.openml.org/r/1849427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, SelectFromModel, RFE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# We don't need a scalar since the dataset is already normalized\n",
    "\n",
    "# Create classifier\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=2048, min_samples_split=4, criterion=\"entropy\", n_jobs=-1)\n",
    "\n",
    "# Build pipeline\n",
    "pipe = Pipeline([\n",
    "    ('classifier', clf)\n",
    "])\n",
    "\n",
    "task = oml.tasks.get_task(145677)\n",
    "print(\"Running..\")\n",
    "run = oml.runs.run_task(task, pipe)\n",
    "print(\"Publishing run\")\n",
    "myrun = run.publish()\n",
    "print(\"RF on %s: http://www.openml.org/r/%d\" % (data.name, myrun.run_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Random forest results\n",
    "\n",
    "This result was certainly good (0.8842 AUC score), but we did not find any way to significantly improve this, other then increasing the number of estimators even further. \n",
    "\n",
    "### SVM's\n",
    "Hence, we now attempted to use an SVM. We expected the linear svm to do well due to high dimensionality (although the rbf should outperform it due to it's ability of being more flexible). But we still wanted to keep the performance from the RandomForest classifier. Hence we used this as the feature selector before we ran the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 75 candidates, totalling 750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 750 out of 750 | elapsed: 43.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.859742509623\n",
      "{'classifier__gamma': 0.01, 'classifier__C': 10, 'classifier__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create feature selector \n",
    "fs = SelectFromModel(RandomForestClassifier(random_state=1, criterion=\"entropy\"))\n",
    "\n",
    "# Create default classifier\n",
    "clf = SVC()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feature_select', fs),\n",
    "    ('classifier', clf)\n",
    "])\n",
    "\n",
    "grid = {\n",
    "    \"classifier__C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"classifier__kernel\": [\"linear\", \"poly\", \"rbf\"],\n",
    "    \"classifier__gamma\": [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "gs = GridSearchCV(pipe, grid, cv=10, n_jobs=-1, scoring=\"roc_auc\", verbose=1)\n",
    "gs.fit(X, y)\n",
    "\n",
    "print(gs.best_score_)     \n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### SVM results\n",
    "The results from this classification where significantly higher then we expected. And seemed to get close to random forest based classifier.\n",
    "\n",
    "Therefore we attempted to improve the best results further by increasing the number of estimators of the random forest classifier. And to optimize only the rbf kernel further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.863451808888\n"
     ]
    }
   ],
   "source": [
    "# Create feature selector \n",
    "fs = SelectFromModel(RandomForestClassifier(random_state=1, n_estimators=512, criterion=\"entropy\", n_jobs=-1))\n",
    "\n",
    "# Create default classifier\n",
    "clf = SVC(kernel=\"rbf\", C=10, gamma=0.01)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feature_select', fs),\n",
    "    ('classifier', clf)\n",
    "])\n",
    "\n",
    "score = cross_val_score(pipe, X, y, cv=10, n_jobs=-1, scoring=\"roc_auc\").mean()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The results were slightly better, but it did not seem to register on openML. Therefore, we decided to pursue several other classifiers.\n",
    "\n",
    "### Naive Bayes  classifiers\n",
    "\n",
    "Similar to linear SVMs, Gaussian NB should be good at seperating high dimensional datasets as well as possibly lead to better generalization. Hence we attempted these next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.431601510757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create default classifier\n",
    "clf = GaussianNB()\n",
    "\n",
    "score = cross_val_score(clf, X, y, cv=10, n_jobs=-1, scoring=\"roc_auc\").mean()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, the results of the GaussianNB were very dissapointing, and we decided not to pursue it any further. In favor of further optimizations of the tree based/SVM classifiers.\n",
    "\n",
    "We attempted to do this by a grid search with a low estimator value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   45.2s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.859490601566\n",
      "{'max_features': 0.3, 'min_samples_leaf': 1, 'min_samples_split': 10, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create default classifier\n",
    "clf = RandomForestClassifier(random_state=1, \n",
    "                             n_estimators=10, \n",
    "                             n_jobs=-1)\n",
    "\n",
    "grid = {\n",
    "    \"max_features\": [0.1, 1, 0.2, 0.3],\n",
    "    \"min_samples_split\": [2, 5, 10, 0.1, 0.2],\n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "    \"min_samples_leaf\": [1, 10, 0.025, 0.05, 0.1]\n",
    "}\n",
    "gs = GridSearchCV(clf, grid, cv=10, n_jobs=-1, scoring=\"roc_auc\", verbose=1)\n",
    "gs.fit(X, y)\n",
    "print(gs.best_score_)     \n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.876652493184\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=1, n_estimators=2048, max_features=0.3, min_samples_split=10, n_jobs=-1)\n",
    "score = cross_val_score(clf, X, y, cv=10, n_jobs=-1, scoring=\"roc_auc\").mean()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as can be seen- this does not work as well, since the low estimator count does not provide any guarantees when a higher amount estimators."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
